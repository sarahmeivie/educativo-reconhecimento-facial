#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema de An√°lise de Foco e Emo√ß√£o com Reconhecimento Facial
Milestone 2: Captura de V√≠deo e Detec√ß√£o Facial
"""

import cv2
import numpy as np
import json
from datetime import datetime
import sys
import time

# Tentar importar MediaPipe com tratamento de erro
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è  MediaPipe n√£o dispon√≠vel: {e}")
    MEDIAPIPE_AVAILABLE = False

# Tentar importar DeepFace com tratamento de erro
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è  DeepFace n√£o dispon√≠vel: {e}")
    DEEPFACE_AVAILABLE = False

class FacialRecognitionSystem:
    def __init__(self):
        """Inicializa o sistema de reconhecimento facial"""
        # Inicializar captura de v√≠deo
        self.cap = None
        self.is_running = False
        
        # Configura√ß√µes para detec√ß√£o de emo√ß√£o
        self.current_emotion = "Neutro"
        self.emotion_confidence = 0.0
        self.last_emotion_time = 0
        self.emotion_analysis_interval = 2.0  # Analisar emo√ß√£o a cada 2 segundos
        
        # Lista de emo√ß√µes relevantes para atividades educativas
        self.educational_emotions = {
            'happy': {'name': 'Feliz', 'color': (0, 255, 0), 'description': 'Engajado e motivado'},
            'sad': {'name': 'Triste', 'color': (255, 0, 0), 'description': 'Desmotivado ou confuso'},
            'angry': {'name': 'Irritado', 'color': (0, 0, 255), 'description': 'Frustrado com dificuldades'},
            'fear': {'name': 'Ansioso', 'color': (128, 0, 128), 'description': 'Nervoso ou inseguro'},
            'disgust': {'name': 'Desgostoso', 'color': (0, 128, 128), 'description': 'Rejeitando o conte√∫do'},
            'surprise': {'name': 'Surpreso', 'color': (255, 255, 0), 'description': 'Interessado ou confuso'},
            'neutral': {'name': 'Neutro', 'color': (128, 128, 128), 'description': 'Concentrado ou indiferente'}
        }
        
        # Vari√°veis para tracking de estado
        self.frame_count = 0
        self.face_detected = False
        self.face_bbox = None
        
        # Inicializar MediaPipe Face Mesh (se dispon√≠vel)
        if MEDIAPIPE_AVAILABLE:
            try:
                self.mp_face_mesh = mp.solutions.face_mesh
                self.mp_drawing = mp.solutions.drawing_utils
                self.mp_drawing_styles = mp.solutions.drawing_styles
                
                # Configurar o modelo Face Mesh
                self.face_mesh = self.mp_face_mesh.FaceMesh(
                    static_image_mode=False,
                    max_num_faces=1,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                self.mediapipe_available = True
                print("‚úÖ MediaPipe inicializado com sucesso!")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erro ao inicializar MediaPipe: {e}")
                self.mediapipe_available = False
        else:
            self.mediapipe_available = False
            
        # Inicializar detector de rosto OpenCV como fallback
        try:
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            self.opencv_available = True
            print("‚úÖ Detector OpenCV inicializado como fallback!")
        except Exception as e:
            print(f"‚ùå Erro ao inicializar detector OpenCV: {e}")
            self.opencv_available = False
            
        # Configura√ß√µes para DeepFace (se dispon√≠vel)
        if DEEPFACE_AVAILABLE:
            self.emotion_models = ['emotion']
            self.emotion_backends = ['opencv']
            self.deepface_available = True
            print("‚úÖ DeepFace dispon√≠vel para an√°lise de emo√ß√µes!")
        else:
            self.deepface_available = False
            print("‚ö†Ô∏è  DeepFace n√£o dispon√≠vel - usando emo√ß√µes simuladas")
        
    def initialize_camera(self, camera_index=0):
        """Inicializa a c√¢mera"""
        try:
            self.cap = cv2.VideoCapture(camera_index)
            if not self.cap.isOpened():
                raise Exception(f"N√£o foi poss√≠vel abrir a c√¢mera {camera_index}")
            
            # Configurar resolu√ß√£o da c√¢mera
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            
            print("‚úÖ C√¢mera inicializada com sucesso!")
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao inicializar c√¢mera: {e}")
            return False
    
    def detect_emotion(self, frame, face_bbox):
        """Detecta emo√ß√£o no rosto detectado"""
        try:
            if face_bbox is None:
                return "Neutro", 0.0
            
            # Se DeepFace n√£o estiver dispon√≠vel, usar emo√ß√£o simulada
            if not self.deepface_available:
                return self.simulate_emotion()
            
            # Extrair regi√£o do rosto
            x, y, w, h = face_bbox
            face_roi = frame[y:y+h, x:x+w]
            
            if face_roi.size == 0:
                return "Neutro", 0.0
            
            # Analisar emo√ß√£o com DeepFace
            result = DeepFace.analyze(
                face_roi, 
                actions=['emotion'], 
                models=self.emotion_models,
                detector_backend=self.emotion_backends[0],
                enforce_detection=False
            )
            
            if isinstance(result, list):
                result = result[0]
            
            # Obter emo√ß√£o dominante
            emotions = result.get('emotion', {})
            if emotions:
                dominant_emotion = max(emotions, key=emotions.get)
                confidence = emotions[dominant_emotion] / 100.0
                return dominant_emotion, confidence
            
            return "Neutro", 0.0
            
        except Exception as e:
            print(f"Erro na detec√ß√£o de emo√ß√£o: {e}")
            return self.simulate_emotion()
    
    def simulate_emotion(self):
        """Simula emo√ß√£o quando DeepFace n√£o est√° dispon√≠vel"""
        import random
        emotions = list(self.educational_emotions.keys())
        emotion = random.choice(emotions)
        confidence = random.uniform(0.6, 0.9)
        return emotion, confidence
    
    def get_face_bbox_mediapipe(self, results, frame_shape):
        """Extrai bounding box do rosto detectado usando MediaPipe"""
        if not results.multi_face_landmarks:
            return None
        
        # Obter dimens√µes do frame
        height, width = frame_shape[:2]
        
        # Obter landmarks do primeiro rosto
        face_landmarks = results.multi_face_landmarks[0]
        
        # Converter landmarks para coordenadas de pixel
        x_coords = []
        y_coords = []
        
        for landmark in face_landmarks.landmark:
            x = int(landmark.x * width)
            y = int(landmark.y * height)
            x_coords.append(x)
            y_coords.append(y)
        
        # Calcular bounding box
        min_x = max(0, min(x_coords) - 20)
        min_y = max(0, min(y_coords) - 20)
        max_x = min(width, max(x_coords) + 20)
        max_y = min(height, max(y_coords) + 20)
        
        return (min_x, min_y, max_x - min_x, max_y - min_y)
    
    def get_face_bbox_opencv(self, frame):
        """Extrai bounding box do rosto detectado usando OpenCV"""
        if not self.opencv_available:
            return None
            
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        
        if len(faces) > 0:
            # Retornar o primeiro rosto detectado
            x, y, w, h = faces[0]
            return (x, y, w, h)
        
        return None
    
    def draw_face_frame(self, frame, face_bbox, emotion, confidence):
        """Desenha frame ao redor do rosto com informa√ß√µes de emo√ß√£o"""
        if face_bbox is None:
            return frame
        
        x, y, w, h = face_bbox
        
        # Obter informa√ß√µes da emo√ß√£o
        emotion_info = self.educational_emotions.get(emotion, 
            {'name': emotion.title(), 'color': (255, 255, 255), 'description': 'Emo√ß√£o detectada'})
        
        # Desenhar ret√¢ngulo ao redor do rosto
        color = emotion_info['color']
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)
        
        # Desenhar fundo para o texto
        text_bg_height = 60
        cv2.rectangle(frame, (x, y + h), (x + w, y + h + text_bg_height), (0, 0, 0), -1)
        cv2.rectangle(frame, (x, y + h), (x + w, y + h + text_bg_height), color, 2)
        
        # Texto da emo√ß√£o
        emotion_text = f"{emotion_info['name']} ({confidence:.1%})"
        cv2.putText(frame, emotion_text, (x + 5, y + h + 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # Descri√ß√£o da emo√ß√£o
        description_text = emotion_info['description']
        cv2.putText(frame, description_text, (x + 5, y + h + 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return frame
    
    def process_frame(self, frame):
        """Processa um frame para detec√ß√£o facial e emo√ß√£o"""
        # Incrementar contador de frames
        self.frame_count += 1
        
        # Tentar usar MediaPipe primeiro
        if self.mediapipe_available:
            try:
                # Converter BGR para RGB (MediaPipe usa RGB)
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Processar com Face Mesh
                results = self.face_mesh.process(rgb_frame)
                
                # Verificar se rosto foi detectado
                self.face_detected = results.multi_face_landmarks is not None
                
                if self.face_detected:
                    # Obter bounding box do rosto
                    self.face_bbox = self.get_face_bbox_mediapipe(results, frame.shape)
                    
                    # Desenhar landmarks faciais
                    for face_landmarks in results.multi_face_landmarks:
                        # Desenhar malha facial completa
                        self.mp_drawing.draw_landmarks(
                            frame,
                            face_landmarks,
                            self.mp_face_mesh.FACEMESH_CONTOURS,
                            None,
                            self.mp_drawing_styles.get_default_face_mesh_contours_style()
                        )
                        
                        # Desenhar √≠ris e pupilas
                        self.mp_drawing.draw_landmarks(
                            frame,
                            face_landmarks,
                            self.mp_face_mesh.FACEMESH_IRISES,
                            None,
                            self.mp_drawing_styles.get_default_face_mesh_iris_connections_style()
                        )
                else:
                    self.face_bbox = None
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Erro no MediaPipe: {e}")
                self.mediapipe_available = False
                # Fallback para OpenCV
                self.face_bbox = self.get_face_bbox_opencv(frame)
                self.face_detected = self.face_bbox is not None
                results = None
        else:
            # Usar OpenCV como fallback
            self.face_bbox = self.get_face_bbox_opencv(frame)
            self.face_detected = self.face_bbox is not None
            results = None
        
        # Detectar emo√ß√£o se rosto foi detectado
        if self.face_detected:
            # Detectar emo√ß√£o periodicamente
            current_time = time.time()
            if current_time - self.last_emotion_time >= self.emotion_analysis_interval:
                emotion, confidence = self.detect_emotion(frame, self.face_bbox)
                self.current_emotion = emotion
                self.emotion_confidence = confidence
                self.last_emotion_time = current_time
            
            # Desenhar frame do rosto com informa√ß√µes de emo√ß√£o
            frame = self.draw_face_frame(frame, self.face_bbox, self.current_emotion, self.emotion_confidence)
        else:
            self.face_bbox = None
            self.current_emotion = "Neutro"
            self.emotion_confidence = 0.0
        
        return frame, results
    
    def run(self):
        """Executa o loop principal do sistema"""
        print("üöÄ Iniciando Sistema de Reconhecimento Facial...")
        print("üìã Milestone 2: Captura de V√≠deo e Detec√ß√£o Facial + Emo√ß√£o")
        print("=" * 60)
        
        # Inicializar c√¢mera
        if not self.initialize_camera():
            return False
        
        self.is_running = True
        print("üìπ C√¢mera ativa. Pressione 'q' para sair...")
        print("üëÅÔ∏è  Aguardando detec√ß√£o facial...")
        print("üòä Sistema de detec√ß√£o de emo√ß√µes ativo!")
        print("üìä Emo√ß√µes detect√°veis:")
        for emotion_key, emotion_data in self.educational_emotions.items():
            print(f"   ‚Ä¢ {emotion_data['name']}: {emotion_data['description']}")
        print("=" * 60)
        
        try:
            while self.is_running:
                # Capturar frame
                ret, frame = self.cap.read()
                if not ret:
                    print("‚ùå Erro ao capturar frame da c√¢mera")
                    break
                
                # Processar frame
                processed_frame, results = self.process_frame(frame)
                
                # Adicionar informa√ß√µes na tela
                height, width = processed_frame.shape[:2]
                
                # Status da detec√ß√£o
                if self.face_detected:
                    status_text = "‚úÖ Rosto Detectado"
                    status_color = (0, 255, 0)  # Verde
                else:
                    status_text = "‚ùå Nenhum Rosto Detectado"
                    status_color = (0, 0, 255)  # Vermelho
                
                # Painel de informa√ß√µes no canto superior esquerdo
                panel_width = 300
                panel_height = 120
                cv2.rectangle(processed_frame, (10, 10), (panel_width, panel_height), (0, 0, 0), -1)
                cv2.rectangle(processed_frame, (10, 10), (panel_width, panel_height), (255, 255, 255), 2)
                
                # Status da detec√ß√£o
                cv2.putText(processed_frame, status_text, (20, 35), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)
                
                # Informa√ß√µes de emo√ß√£o
                if self.face_detected:
                    emotion_info = self.educational_emotions.get(self.current_emotion, 
                        {'name': self.current_emotion.title(), 'color': (255, 255, 255)})
                    
                    emotion_text = f"Emocao: {emotion_info['name']}"
                    cv2.putText(processed_frame, emotion_text, (20, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, emotion_info['color'], 1)
                    
                    confidence_text = f"Confianca: {self.emotion_confidence:.1%}"
                    cv2.putText(processed_frame, confidence_text, (20, 80), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                    
                    frames_text = f"Frames: {self.frame_count}"
                    cv2.putText(processed_frame, frames_text, (20, 100), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                
                # Painel de emo√ß√µes dispon√≠veis no canto superior direito
                emotions_panel_x = width - 250
                emotions_panel_y = 10
                emotions_panel_width = 240
                emotions_panel_height = 200
                
                cv2.rectangle(processed_frame, (emotions_panel_x, emotions_panel_y), 
                             (emotions_panel_x + emotions_panel_width, emotions_panel_y + emotions_panel_height), 
                             (0, 0, 0), -1)
                cv2.rectangle(processed_frame, (emotions_panel_x, emotions_panel_y), 
                             (emotions_panel_x + emotions_panel_width, emotions_panel_y + emotions_panel_height), 
                             (255, 255, 255), 2)
                
                cv2.putText(processed_frame, "EMOCOES DETECTAVEIS:", (emotions_panel_x + 10, emotions_panel_y + 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                y_offset = 45
                for i, (emotion_key, emotion_data) in enumerate(self.educational_emotions.items()):
                    if y_offset > emotions_panel_height - 20:
                        break
                    
                    # Destacar emo√ß√£o atual
                    if emotion_key == self.current_emotion:
                        cv2.rectangle(processed_frame, (emotions_panel_x + 5, emotions_panel_y + y_offset - 15), 
                                     (emotions_panel_x + emotions_panel_width - 5, emotions_panel_y + y_offset + 5), 
                                     emotion_data['color'], -1)
                        text_color = (0, 0, 0)
                    else:
                        text_color = emotion_data['color']
                    
                    emotion_display = f"‚Ä¢ {emotion_data['name']}"
                    cv2.putText(processed_frame, emotion_display, (emotions_panel_x + 10, emotions_panel_y + y_offset), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, text_color, 1)
                    y_offset += 20
                
                # Informa√ß√µes na parte inferior
                cv2.putText(processed_frame, "Sistema de Analise de Foco e Emocao", 
                           (10, height - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                cv2.putText(processed_frame, "Pressione 'q' para sair", 
                           (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                cv2.putText(processed_frame, "Milestone 2: Deteccao Facial e Emocao", 
                           (10, height - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
                
                # Exibir frame
                cv2.imshow('Sistema de Reconhecimento Facial - Milestone 2', processed_frame)
                
                # Verificar tecla de sa√≠da
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("üõë Encerrando sistema...")
                    break
                    
        except KeyboardInterrupt:
            print("\nüõë Sistema interrompido pelo usu√°rio")
        except Exception as e:
            print(f"‚ùå Erro durante execu√ß√£o: {e}")
        finally:
            self.cleanup()
        
        return True
    
    def cleanup(self):
        """Limpa recursos do sistema"""
        self.is_running = False
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        print("üßπ Recursos liberados com sucesso!")

def main():
    """Fun√ß√£o principal"""
    print("=" * 60)
    print("üéØ PROT√ìTIPO DE AN√ÅLISE DE FOCO E EMO√á√ÉO")
    print("üìä Sistema de Reconhecimento Facial")
    print("=" * 60)
    
    # Verificar se as bibliotecas b√°sicas est√£o instaladas
    try:
        import cv2
        import numpy as np
        print("‚úÖ Bibliotecas b√°sicas (OpenCV, NumPy) dispon√≠veis")
    except ImportError as e:
        print(f"‚ùå Erro de importa√ß√£o: {e}")
        print("üí° Execute: pip install opencv-python numpy")
        return False
    
    # Mostrar status das bibliotecas opcionais
    if MEDIAPIPE_AVAILABLE:
        print("‚úÖ MediaPipe dispon√≠vel - detec√ß√£o facial avan√ßada")
    else:
        print("‚ö†Ô∏è  MediaPipe n√£o dispon√≠vel - usando OpenCV como fallback")
    
    if DEEPFACE_AVAILABLE:
        print("‚úÖ DeepFace dispon√≠vel - an√°lise de emo√ß√µes real")
    else:
        print("‚ö†Ô∏è  DeepFace n√£o dispon√≠vel - usando emo√ß√µes simuladas")
    
    print("=" * 60)
    
    # Criar e executar sistema
    system = FacialRecognitionSystem()
    success = system.run()
    
    if success:
        print("‚úÖ Milestone 2 conclu√≠do com sucesso!")
        print("üéØ Funcionalidades implementadas:")
        print("   ‚Ä¢ Detec√ß√£o facial em tempo real")
        if DEEPFACE_AVAILABLE:
            print("   ‚Ä¢ An√°lise de emo√ß√µes com DeepFace")
        else:
            print("   ‚Ä¢ Simula√ß√£o de emo√ß√µes")
        print("   ‚Ä¢ Interface visual melhorada")
        print("   ‚Ä¢ Frame destacado no rosto")
        print("   ‚Ä¢ Lista de emo√ß√µes educativas")
        print("üìã Pr√≥ximo passo: Implementar calibra√ß√£o do olhar")
    else:
        print("‚ùå Falha na execu√ß√£o do Milestone 2")
    
    return success

if __name__ == "__main__":
    main()
